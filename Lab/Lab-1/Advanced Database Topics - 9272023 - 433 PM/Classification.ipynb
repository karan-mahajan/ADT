{"cells":[{"cell_type":"markdown","metadata":{"id":"jG0mSN6QSBzL"},"source":["# Classification\n","\n","**Dataset:** Spambase \n","\n"]},{"cell_type":"markdown","metadata":{"id":"C8rB-p6-SBzN"},"source":["# IMPORT LIBRARIES"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E4RthhB6SBzO"},"outputs":[],"source":["import pandas as pd                                   # For dataframes\n","import matplotlib.pyplot as plt                       # For plotting data\n","import seaborn as sns                                 # For plotting data\n","from sklearn.model_selection import train_test_split  # For train/test splits\n","from sklearn.model_selection import GridSearchCV     # For parameter optimization\n","from sklearn.neighbors import KNeighborsClassifier   # For kNN classification\n","from sklearn.metrics import plot_confusion_matrix    # Evaluation measure"]},{"cell_type":"markdown","metadata":{"id":"287GrAJ6SBzP"},"source":["# LOAD AND PREPARE DATA\n","Many of the datasets for this course come from the Machine Learning Repository at the University of California, Irvine (UCI) at [https://archive.ics.uci.edu/](https://archive.ics.uci.edu/).\n","\n","For this demonstrations of clustering techniques, we'll use the `Spambase Data Set`, which can be accessed via [https://archive.ics.uci.edu/ml/datasets/Spambase](https://archive.ics.uci.edu/ml/datasets/Spambase). We'll use the dataset saved in the file `spambase.data`. \n","\n","This data can be downloaded as a `CSV` file without the variable names using `pd.read_csv`. You'll need to manually add the `.csv` extension. This code saves the file in the data folder of our Python directory."]},{"cell_type":"markdown","metadata":{"id":"A69hnwh9SBzP"},"source":["## Import Data"]},{"cell_type":"markdown","metadata":{"id":"t_HqsvUnSBzQ"},"source":["- To read read the dataset from a local CSV file, run the following cell. (This is the recommended approach.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bCOW5LXWSBzQ"},"outputs":[],"source":["df = pd.read_csv('data/spambase_raw.csv', header=None)"]},{"cell_type":"markdown","metadata":{"id":"woiD4dgMSBzQ"},"source":["- Alternatively, to read the data from the UCI ML Repository, uncomment the lines in the cell below and run them."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jeFw-aw_SBzQ"},"outputs":[],"source":["\n","df = pd.read_csv(\n","    'https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data',\n","    header=None)"]},{"cell_type":"markdown","metadata":{"id":"tj7N2mFwSBzR"},"source":["- Look at the data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cTWtpSz0SBzR"},"outputs":[],"source":["df.head()"]},{"cell_type":"markdown","metadata":{"id":"l2tM5F7fSBzR"},"source":["## Rename Variables\n","\n","- Assign a name to all attributes as `X0`, `X1`, ..., `X56`.\n","- Assign `y` to the class variable (the last column of df).\n","- Display the first 5 rows."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n83I9ubcSBzR"},"outputs":[],"source":["# Sequentially renames all attribute columns and renames the last column to 'y'\n","df.columns = ['X' + str(i) for i in range(0, len(df.columns) - 1)] + ['y']\n","\n","# Shows the first few lines of the data\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"aHg_fyjWSBzS"},"source":["## Split Data\n","To prepare the dataset for classification, we have to split it into train and test sets.\n","\n","- `train_test_split()` splits the data into train and test.\n","- In the arguments list, the data matrix consists of all attribute columns. Extract columns `X0`, `X1`, ..., `X56` with `df.filter(regex='\\d')`. The filter keeps only the names that have a numeric character in them.\n","- Specify the target variable as `df.y`.\n","- Set up `trn` and `tst` dataframes."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fY-NOTM9SBzS"},"outputs":[],"source":["# Specifies X by filtering all columns with a number in name\n","X_trn, X_tst, y_trn, y_tst = train_test_split(\n","    df.filter(regex='\\d'),  \n","    df.y, \n","    test_size=0.30,\n","    random_state=1)\n","\n","# Creates the training dataset, trn\n","trn = X_trn\n","trn['y'] = y_trn\n","\n","# Creates the testing dataset, tst\n","tst = X_tst\n","tst['y'] = y_tst"]},{"cell_type":"markdown","metadata":{"id":"eJQSXZttSBzS"},"source":["# EXPLORE TRAINING DATA"]},{"cell_type":"markdown","metadata":{"id":"HlAt2bXkSBzT"},"source":["## Bar Plot of Class Variable\n","\n","Use Seaborn's `countplot()` function to create a bar plot."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"_fQc65hASBzT"},"outputs":[],"source":["sns.countplot(x='y', data=trn)"]},{"cell_type":"markdown","metadata":{"id":"OT7IAw7JSBzT"},"source":["## Explore Attribute Variables\n","Select four arbitrary features and get paired plots (takes a moment)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H5FCbOObSBzT"},"outputs":[],"source":["# Creates a grid using Seaborn's PairGrid()\n","g = sns.PairGrid(\n","    trn, \n","    vars=['X5', 'X20', 'X25', 'X53'], \n","    hue='y', \n","    diag_sharey=False, \n","    palette=['red', 'green'])\n","\n","# Adds histograms on the diagonal\n","g.map_diag(plt.hist)\n","\n","# Adds density plots above the diagonal\n","g.map_upper(sns.kdeplot)\n","\n","# Adds scatterplots below the diagonal\n","g.map_lower(sns.scatterplot)\n","\n","# Adds a legend\n","g.add_legend(title='Spam')\n"]},{"cell_type":"markdown","source":["##PREPARE DATA\n","\n","Separate the data matrix from the class variable."],"metadata":{"id":"72GEG1BldNFM"}},{"cell_type":"code","source":["# Separates the attributes X0-X56 into X_trn\n","X_trn = trn.filter(regex='\\d')\n","\n","# Separates the class variable into y_trn\n","y_trn = trn.y\n","\n","# Separates the attributes X0-X56 into X_tst\n","X_tst = tst.filter(regex='\\d')\n","\n","# Separates the class variable into y_tst\n","y_tst = tst.y\n","\n","# Class labels\n","spam = ['Not Spam','Spam']"],"metadata":{"id":"JiBcIx0IVfSc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trn.head()"],"metadata":{"id":"bQWC2cT6XBhS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## kNN: TRAIN MODEL\n","To train a kNN model, set up a KNeighborsClassifier object and fit it to training data.\n","\n"],"metadata":{"id":"QPOSQXdUdZxh"}},{"cell_type":"code","source":["# Sets up a kNN model and fits it to data\n","knn = KNeighborsClassifier(n_neighbors=5) \\\n","    .fit(X_trn, y_trn)\n"],"metadata":{"id":"ahskhPPIXNQw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Calculate Mean Accuracy on Training Data"],"metadata":{"id":"AIMCinurdmb-"}},{"cell_type":"code","source":["print(\n","    'Accuracy on training data: ' \n","    + str(\"{:.2%}\".format(knn.score(X_trn, y_trn))))"],"metadata":{"id":"L6bXPMZcZG_E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Optimize the kNN Model\n","The challenge in training a kNN model is to determine the optimal number of neighbors. To find the optimal parameters, GridSearchCV object can be used."],"metadata":{"id":"l_XnP3-8dr71"}},{"cell_type":"code","source":["# Sets up the kNN classifier object\n","knn = KNeighborsClassifier() \n","\n","# Search parameters\n","param = range(3, 15, 2)\n","\n","# Sets up GridSearchCV object and stores it in grid variable\n","grid = GridSearchCV(\n","    knn,\n","    {'n_neighbors': param})\n","\n","# Fits the grid object and gets the best model\n","best_knn = grid \\\n","    .fit(X_trn,y_trn) \\\n","    .best_estimator_\n","\n","# Displays the optimum model\n","best_knn.get_params()"],"metadata":{"id":"l3Jj1ZhEZf-A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Plot the Accuracy by Neighbors Parameter\n","Once the optimal parameters are found, the accuracy for different parameters can be compared by plotting. The grid variable has an attribute cv_results_, which is a dictionary of key value pairs and stores the cross validation accuracy for each parameter."],"metadata":{"id":"hkzTd84TdwhW"}},{"cell_type":"code","source":["# Plots mean_test_scores vs. total neighbors\n","plt.plot(\n","    param,\n","    grid.cv_results_['mean_test_score'])\n","\n","# Adds labels to the plot\n","plt.xticks(param)\n","plt.ylabel('Mean CV Score')\n","plt.xlabel('n_neighbors')\n","\n","# Draws a vertical line where the best model is\n","plt.axvline(\n","    x=best_knn.n_neighbors, \n","    color='red', \n","    ls='--')"],"metadata":{"id":"vDGVfsokbQ4x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##TEST MODEL\n","In this phase, we'll evaluate the accuracy of the trained kNN model on the test set. A good evaluation measure is the confusion matrix that gives the fraction of true positives, true negatives, false positives, and false negatives.\n","\n","###Visualize the Confusion Matrix\n","Normalize the scores to display as proportions across rows."],"metadata":{"id":"jG0CVVI1d2C7"}},{"cell_type":"code","source":["plot_confusion_matrix(\n","    best_knn, X_tst, y_tst,\n","    display_labels=spam,\n","    normalize='true')"],"metadata":{"id":"DZ84gHwbb25f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Calculate Mean Accuracy on Testing Data"],"metadata":{"id":"7c1_sZfDd9SB"}},{"cell_type":"code","source":["print(\n","    'Accuracy on testing data: ' \n","    + str(\"{:.2%}\".format(best_knn.score(X_tst, y_tst))))"],"metadata":{"id":"uqhbozCfcjdO"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}